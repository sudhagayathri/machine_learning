{
 "cells": [
  {
   "cell_type": "raw",
   "id": "aae30e7b",
   "metadata": {},
   "source": [
    "does decision tree comes under bagging technique? what is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773da61",
   "metadata": {},
   "source": [
    "\n",
    "A decision tree is a supervised learning algorithm that can be used for both classification and regression tasks. It works by partitioning the data into smaller subsets based on specific features until it reaches a leaf node, which contains the final prediction.\n",
    "\n",
    "Bagging (Bootstrap Aggregating) is an ensemble learning technique that aims to improve the performance of machine learning algorithms. It involves training multiple instances of the same base learning algorithm on different subsets of the training data. Each subset is generated by random sampling with replacement (bootstrap sampling). The final prediction is obtained by averaging (in regression) or taking a vote (in classification) from the predictions of these individual models.\n",
    "\n",
    "One popular implementation of bagging is the Random Forest algorithm, which builds multiple decision trees using bootstrapped subsets of the data and random subsets of features.\n",
    "\n",
    "In summary, while decision trees are a type of base learning algorithm, bagging is an ensemble technique that can be applied to various base learners, including decision trees.\n",
    "\n",
    "\n",
    "Decision trees Hyper-params:\n",
    "criterion\n",
    "depth\n",
    "min_sample_split\n",
    "min_sample_leaf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d5b92bc",
   "metadata": {},
   "source": [
    "Gini Gain, Information Gain and Gini Index?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48e48f",
   "metadata": {},
   "source": [
    "**The Gini index** is a measure of impurity that calculates the probability of misclassifying a randomly chosen element from the dataset. It is calculated by subtracting the sum of squared probabilities of each class from one. The Gini index favors larger distributions and is used by CART algorithms12.\n",
    "\n",
    "On the other hand, **Gini gain** is a measure of the reduction in impurity achieved by splitting a dataset using a specific attribute. It is calculated by subtracting the weighted average of the Gini indices of the resulting child nodes from the Gini index of the parent node. Gini gain helps determine which attribute provides the most significant reduction in impurity when creating decision tree splits.\n",
    "\n",
    "On the other hand, **information gain** is a measure of the reduction in entropy achieved by splitting a dataset. Entropy is a measure of impurity that calculates the degree of randomness or uncertainty in a dataset. Information gain favors smaller distributions with multiple specific values and is used by ID3 and C4.5 algorithms 123."
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa75642c",
   "metadata": {},
   "source": [
    "Bias and variance in model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df84e8",
   "metadata": {},
   "source": [
    "**high bias mean:**\n",
    "your model is highly biased towards one or two features and giving prediction based on those highly biased features and neglecting other features, it means that your model didn't learn all the features well so you can say it's is Underfitted model.\n",
    "\n",
    "**High variance means:**\n",
    "when  your model  learn all the features to well\n",
    "and even the outliers or noise \n",
    "it may arise Overfitting \n",
    "\n",
    "**High Bias and Low Variance:**\n",
    "it means importance is given to  one or two features and ignoring others(high Biased) , \n",
    "not giving other things any importance (Less variance).  it is Underfitted condition\n",
    "\n",
    "**Low biased High variance:**\n",
    " importance is given to every features and even noise(high variance) , not biased towards one or two features(low bias). everything is used for prediction\n",
    "so it's Overfitting condition  \n",
    "\n",
    "that why we always look for low bias and low  Variance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57d5d606",
   "metadata": {},
   "source": [
    "KNN ALgorithm(k-nearest neighbour) - classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98615e18",
   "metadata": {},
   "source": [
    "**KNN ALgorithm(k-nearest neighbour)** is instance based algoritm(learning data by heart) instead of supervised learning\n",
    "here classification is done by choosing k-nearest neighbor and by chossing majority class\n",
    "\n",
    "To classify unknown record:\n",
    "compute dst to other records\n",
    "identify k nearest neighbors\n",
    "use class label of nearest neighbor\n",
    "\n",
    "Dst calculated based on Euclidean distance\n",
    "\n",
    "It is called as  **Lazy Learner** - bcoz for each input model shud execute to calculate distance/plot \n",
    "Retraining of model shud be done"
   ]
  },
  {
   "cell_type": "raw",
   "id": "487b3f0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3742c305",
   "metadata": {},
   "source": [
    "**Reasons for overfitting in Decison Tree:**\n",
    "\n",
    "\n",
    "1. Hyper parameters\n",
    "2. SAmpling - this can be solved by cross-validation technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a258072",
   "metadata": {},
   "source": [
    "**Pruning in DT**\n",
    "\n",
    "\n",
    "Growing large tree and merging them backnodes/cutting them\n",
    "It is hit and try method to get accuracy improved over depth of tree gettig reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85eef9d",
   "metadata": {},
   "source": [
    "**roc-auc curve**:\n",
    "\n",
    "\n",
    "it is one of the evaluaiton metrics\n",
    "it is a graph btn TPR and FPR\n",
    "Roc s receiver Operating characterisitcs curve\n",
    "auc s area under the roc curve\n",
    "The higher the auc the better is the model\n",
    "auc rangs from  0 to 1\n",
    "it means classifier can corectly distinguish classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fae2b6",
   "metadata": {},
   "source": [
    "**Random forest:**\n",
    "\n",
    "It is a ensemble technique(Bagging) which combines multiple decison tree to provide better accurate & stable data\n",
    "it basically creates m decision tree from D samples with replacement\n",
    "\n",
    "Rf error rate:\n",
    "1.correlation btn trees\n",
    "2.strngth of individual tree\n",
    "3.no of varaibles/cols per tree\n",
    "\n",
    "\n",
    "Hyper-params:\n",
    "bootstrap - sampling with replacement - true/false\n",
    "n_estimators - no of decision trees to be created\n",
    "max_features - max no of features per tree\n",
    "criterion\n",
    "min_sample_split\n",
    "max_leaf_node\n",
    "\n",
    "OOB rate:\n",
    "avg loss calcualted on each tree over entire forest\n",
    "we take n_estimators where oob rate is low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32c92b",
   "metadata": {},
   "source": [
    "**Logistic regression:** \n",
    "\n",
    "\n",
    "is a supervised machine learning algorithm1. It is mainly used for classification tasks where the goal is to predict the probability that an instance belongs to a given class or not\n",
    "\n",
    "Probablistic model - Odds ratio = p/1-p\n",
    "sigmoid fn\n",
    "s-curve\n",
    "binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31647a1",
   "metadata": {},
   "source": [
    "**Support Vector Machine (SVM)** \n",
    "\n",
    "\n",
    "is a supervised machine learning algorithm12. It is primarily used for classification problems in machine learning1. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes1. This decision boundary is called a hyperplane1.\n",
    "\n",
    "SVM chooses the extreme points/vectors, known as support vectors, that help in creating the hyperplane1. By using these support vectors, SVM can classify new data points into the correct category based on their position relative to the hyperplane1.\n",
    "\n",
    "Decision Boundary - created by choosing most nearest classes that are opposite\n",
    "\n",
    "can handle non-linear data\n",
    "z=x2+y2\n",
    "kernek trick - by adding higher dimensions it makes our data separable\n",
    "\n",
    "Hyper params: \n",
    "c-regularisation \n",
    "kernel\n",
    "gamma -  how far is influence of single training param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd0d45",
   "metadata": {},
   "source": [
    "**ADa Boost**\n",
    "\n",
    "Boosting technique where decision tress are trained sequntially\n",
    "\n",
    "Each datapoint is assigned same wts\n",
    "stumps\n",
    "new wts is assigned with wrong data getting higher wt\n",
    "Each d.t is fed with maximum no of wrongly predicted records\n",
    "\n",
    "Hyper param - n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec6612",
   "metadata": {},
   "source": [
    "**Gradient and xgboost:**\n",
    "\n",
    "It boosts basemodel(fixed ops) with the helps of residuals(losses) by adding Decison trees sequentially\n",
    "\n",
    "Residulas are multiplied with learning rate\n",
    "\n",
    "o.p of d.t are residuals\n",
    "\n",
    "\n",
    "**adv of xgboost over gradient :**\n",
    "\n",
    "Regularization: XGBoost uses a regularization technique that helps prevent overfitting and improves model generalization1.\n",
    "\n",
    " In tree-based methods regularization is usually understood as defining a minimum gain so which another split happens:.\n",
    "\n",
    "\n",
    "Parallelization: XGBoost supports parallel processing, making it faster than traditional gradient boosting algorithms1.\n",
    "\n",
    "\n",
    "Missing Values Handling: XGBoost has built-in handling for missing values, reducing the need for preprocessing steps4.\n",
    "\n",
    "Scalability: XGBoost is designed to handle large datasets efficiently, making it suitable for big data applications1.\n",
    "\n",
    "Customization: XGBoost provides a wide range of hyperparameters that can be fine-tuned to optimize model performance1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec3f6d",
   "metadata": {},
   "source": [
    "**GridSearchCV and RandomizedSearchCV**\n",
    "\n",
    "They are used in hyperparameter optimization for machine learning models.\n",
    "\n",
    "GridSearchCV evaluates the model for all possible hyperparameter combinations in a grid over the search space.\n",
    "RandomizedSearchCV uses a random set of hyperparameters to evaluate the model.\n",
    "\n",
    "Grid - when searchspace is small\n",
    "Random - when searchspace is large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b8338",
   "metadata": {},
   "source": [
    "**random_state** parameter in the train_test_split function is used to control the shuffling applied to the data before splitting it into training and testing datasets123. It ensures that the same randomization is used each time you run the code, resulting in the same splits of the data2.\n",
    "\n",
    "Results in same dataset and is reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe83ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f69cd9a",
   "metadata": {},
   "source": [
    "For **health care domian and research data set**, knn and svm are preferable as they are white box approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ad2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1923937b",
   "metadata": {},
   "source": [
    "**why linear regression is not suitable for classification in machine learning**\n",
    "\n",
    "Linear Regression is not suitable for classification in machine learning because it deals with continuous values, whereas classification problems mandate discrete values.\n",
    "\n",
    "Additionally, **the shift in threshold value**  when new data points are added is another problem. For instance, if we have to classify whether a person is sick or not, this method is not reliable at all since we use a sigmoid function in logistic regression. If there is a shift in the line, then the minimum threshold value for classifying the examples changes. So in the beginning, the threshold might be 0.5 but after adding few more data points, the value might be 0.8. So the accuracy of the model takes a hit here. Hence to avoid this, we use an algorithm called Logistic Regression which is a binary classification algorithm to step over these practical problems that hold back Linear Regression for classification \n",
    "\n",
    "\n",
    "\n",
    "Also best fit line shud keep changing once new points are added in dataset\n",
    "and when compared with Logistic it gives larger rmse(root mean square error) as best fit line is not suitable\n",
    "for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363092a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "412f6eb6",
   "metadata": {},
   "source": [
    "**what are assumptions of logistic regression?**\n",
    "\n",
    "The response variable is binary.\n",
    "\n",
    "The observations are independent.\n",
    "\n",
    "There is no multicollinearity among explanatory variables.\n",
    "\n",
    "There are no extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f704eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3329d56e",
   "metadata": {},
   "source": [
    "**Bias:** Bias refers to the error due to overly simplistic assumptions in the learning algorithm. These assumptions make the model easier to comprehend and learn but might not capture the underlying complexities of the data. It is the error due to the model’s inability to represent the true relationship between input and output accurately. \n",
    "\n",
    "When a model has poor performance both on the training and testing data means high bias because of the simple model, indicating **underfitting**.\n",
    "\n",
    "To address underfitting problem of the model, we need to use more complex models, with enhanced feature representation, and **less regularization.**\n",
    "\n",
    "\n",
    "**Variance:** Variance, on the other hand, is the error due to the model’s sensitivity to fluctuations in the training data. It’s the variability of the model’s predictions for different instances of training data. High variance occurs when a model learns the training data’s noise and random fluctuations rather than the underlying pattern. \n",
    "\n",
    "As a result, the model performs well on the training data but poorly on the testing data, indicating **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471760e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80fa0f51",
   "metadata": {},
   "source": [
    "**Why is AUC a better measure of an algorithm's performance than accuracy?**\n",
    "\n",
    "For binary classification problem, AUC, graph of true positive vs false positive gives information about how your classifier is as compared to random guesses. If classifier is good AUC will be close to 1. AUC is better measure of classifier performance than accuracy because it does not bias on size of test or evaluation data. Accuracy is always biased on size of test data. In most of the cases, we use 20% data as evaluation or test data for our algorithm of total training data. In such cases AUC is better measure of accuracy than accuracy. Read more about ROC curve (Receiver Operating Characteristics Curve) to learn how AUC can be used in practice.\n",
    "\n",
    "\n",
    "**auc  of 0.5 is not good**\n",
    "\n",
    "This ROC curve has an AUC of 0.5, meaning it ranks a random positive example higher than a random negative example 50% of the time. As such, the corresponding classification model is basically worthless, as its predictive ability is no better than random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650dddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d12eda66",
   "metadata": {},
   "source": [
    "**Difference between standarization and normalisation**\n",
    "\n",
    "Both are used for **feature scaling** only, where the scales of different features are brought to same level, so\n",
    "that too much importance is not given to single feature\n",
    "\n",
    "**Normalization or Min-Max Scaling:**\n",
    "\n",
    "uses min-max scaler \n",
    "X_new = (X - X_min)/(X_max - X_min)\n",
    "This scales the range to [0, 1]\n",
    "normally used when there are no outliers, as it gets affected by outliers\n",
    "\n",
    "**Standardization or Z-Score Normalization:**\n",
    "\n",
    "uses StandardScaler\n",
    "done by z-score X_new = (X - mean)/Std\n",
    "It is used when we want to ensure zero mean and unit standard deviation.\n",
    "less affected by outliers\n",
    "\n",
    "\n",
    "**feature scaling is required only for elcuidean distance based models/models that require global minima**\n",
    "\n",
    "**Required for models like Linear, Logistic,knn, k-means**\n",
    "\n",
    "**Not required for tree based models**\n",
    "\n",
    "**Standard sclaer performs better for normal machine learning**\n",
    "**for image models like cnn where data needs to be btn 0-1, minmax is required**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2c2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a3944fa",
   "metadata": {},
   "source": [
    "**Standard Deviation**\n",
    "It is the measure of the dispersion of values in a given data set relative to their mean.\n",
    "it gives absolute variability of data set\n",
    "used for market volatality\n",
    "\n",
    "**Variance**\n",
    "It is the statistical measure of how far the numbers are spread in a data set from their average.\n",
    "it gives size of data spread\n",
    "used for asset allocation etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfcdc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91defa2a",
   "metadata": {},
   "source": [
    "**fit, transorm and fit_transform**\n",
    "\n",
    "fit()\tLearn and estimate the parameters of the transformation\testimator.\n",
    "fit(X)\testimator.fit(train_data)\n",
    "\n",
    "transform()\tApply the learned transformation to new data\t\n",
    "transformed_data = estimator.transform(X)\t\n",
    "transformed_data = estimator.transform(test_data)\n",
    "\n",
    "fit_transform()\t\n",
    "Learn the parameters and apply the transformation to new data\t\n",
    "transformed_data = estimator.fit_transform(X)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30345c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc6328a7",
   "metadata": {},
   "source": [
    "**Anomaly Detection** is the technique of identifying rare events or observations which can raise suspicions by being statistically different from the rest of the observations. Such “anomalous” behaviour typically translates to some kind of a problem like a credit card fraud, failing machine in a server, a cyber attack, etc.\n",
    "An anomaly can be broadly categorized into three categories –\n",
    " \n",
    "\n",
    "Point Anomaly: A tuple in a dataset is said to be a Point Anomaly if it is far off from the rest of the data.\n",
    "\n",
    "Contextual Anomaly: An observation is a Contextual Anomaly if it is an anomaly because of the context of the observation.\n",
    "\n",
    "Collective Anomaly: A set of data instances help in finding an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f4f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f12ea9",
   "metadata": {},
   "source": [
    "**Difference btn gradient boosting and gradient descent**\n",
    "\n",
    "In gradient descent, the algorithm optimizes the parameters of a single model to minimize a cost function.\n",
    "\n",
    "In contrast, gradient boosting aims to optimize an ensemble of models by iteratively adding new models to minimize the cost function of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6997ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b75e75a6",
   "metadata": {},
   "source": [
    "**multi class in logistic**\n",
    "\n",
    "one vs all approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
